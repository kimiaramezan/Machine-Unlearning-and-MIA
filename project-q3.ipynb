{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-03T06:50:20.231945Z","iopub.status.busy":"2024-07-03T06:50:20.231470Z","iopub.status.idle":"2024-07-03T06:50:26.744392Z","shell.execute_reply":"2024-07-03T06:50:26.743461Z","shell.execute_reply.started":"2024-07-03T06:50:20.231915Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:51:29.779891Z","iopub.status.busy":"2024-07-03T06:51:29.779074Z","iopub.status.idle":"2024-07-03T06:51:29.788249Z","shell.execute_reply":"2024-07-03T06:51:29.787225Z","shell.execute_reply.started":"2024-07-03T06:51:29.779852Z"},"trusted":true},"outputs":[],"source":["class CIFAR10Classifier(nn.Module):\n","    def __init__(self):\n","        super(CIFAR10Classifier, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n","        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        self.fc1 = nn.Linear(6272, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:51:33.041646Z","iopub.status.busy":"2024-07-03T06:51:33.040720Z","iopub.status.idle":"2024-07-03T06:51:45.663728Z","shell.execute_reply":"2024-07-03T06:51:45.662824Z","shell.execute_reply.started":"2024-07-03T06:51:33.041605Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:08<00:00, 20971200.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]},{"data":{"text/plain":["60000"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","dataset = ConcatDataset([train_dataset, test_dataset])\n","len_dataset = len(dataset)\n","\n","model = CIFAR10Classifier()\n","state_dict = torch.load(\"/kaggle/input/private-model/model_state_dict.pth\")\n","new_state_dict = {}\n","for key, value in state_dict.items():\n","    new_key = key.replace('_module.', '')  \n","    new_state_dict[new_key] = value\n","\n","model.load_state_dict(new_state_dict)\n","model = model.to(device)\n","model.eval()\n","len_dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:51:48.131807Z","iopub.status.busy":"2024-07-03T06:51:48.131456Z","iopub.status.idle":"2024-07-03T06:52:37.625426Z","shell.execute_reply":"2024-07-03T06:52:37.624494Z","shell.execute_reply.started":"2024-07-03T06:51:48.131780Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"data":{"text/plain":["12252"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def generate_synthetic_data(private_model, conf_min):\n","    private_model.eval()\n","    images, labels = [], []\n","    with torch.no_grad():\n","        for i in range(len_dataset):\n","            x,target = dataset[i]\n","            x = x.to(device)\n","            y = private_model(x.unsqueeze(0)) \n","            y = torch.nn.functional.softmax(y, dim=1)\n","            prob = torch.max(y).item()\n","            \n","            if prob > conf_min:\n","                    images.append(x) , labels.append(target)\n","                    \n","    images = torch.stack(images)\n","    labels = torch.tensor(labels)\n","    \n","    return images, labels\n","\n","class Synthetic(Dataset):\n","    def __init__(self, images, labels):\n","        self.images = images\n","        self.labels = labels\n","    \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        return self.images[idx], self.labels[idx]\n","\n","images, labels = generate_synthetic_data(model, 0.6)\n","synthetic_dataset = Synthetic(images, labels)\n","len(synthetic_dataset)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:52:43.583587Z","iopub.status.busy":"2024-07-03T06:52:43.582979Z","iopub.status.idle":"2024-07-03T06:52:43.957795Z","shell.execute_reply":"2024-07-03T06:52:43.956883Z","shell.execute_reply.started":"2024-07-03T06:52:43.583555Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0832656792398763\n"]},{"data":{"text/plain":["0.5111002285341169"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["criterion = nn.CrossEntropyLoss()\n","synthetic_dataloader = DataLoader(synthetic_dataset,batch_size=64,shuffle=True)\n","correct = 0\n","running_loss = 0.0\n","for inputs,labels in synthetic_dataloader:\n","    with torch.no_grad():\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        running_loss += loss.item() * inputs.size(0)\n","        predicted = np.argmax(outputs.cpu().numpy(), axis = 1)\n","        correct += np.sum(predicted == labels.cpu().numpy())\n","print(running_loss / len(synthetic_dataset))\n","correct / len(synthetic_dataset)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:53:32.460921Z","iopub.status.busy":"2024-07-03T06:53:32.460536Z","iopub.status.idle":"2024-07-03T06:54:33.520201Z","shell.execute_reply":"2024-07-03T06:54:33.519113Z","shell.execute_reply.started":"2024-07-03T06:53:32.460881Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.9183851303070705\n","Epoch 2, Loss: 1.7227359379084863\n","Epoch 3, Loss: 1.6377929649838578\n","accuracy :  0.5586026771139406\n","accuracy :  0.5466862553052563\n","Epoch 1, Loss: 1.9089541108427721\n","Epoch 2, Loss: 1.764962779946489\n","Epoch 3, Loss: 1.6607120577100052\n","accuracy :  0.5527260855370552\n","accuracy :  0.5471759712699967\n","Epoch 1, Loss: 1.9188995442253491\n","Epoch 2, Loss: 1.7382448399658303\n","Epoch 3, Loss: 1.6595583544854395\n","accuracy :  0.5287300032647732\n","accuracy :  0.5210577864838394\n","Epoch 1, Loss: 1.9342392897792646\n","Epoch 2, Loss: 1.7187658445642138\n","Epoch 3, Loss: 1.6405471410047914\n","accuracy :  0.5439111981717271\n","accuracy :  0.5156709108716944\n","Epoch 1, Loss: 1.8841518286625336\n","Epoch 2, Loss: 1.6841828297852848\n","Epoch 3, Loss: 1.5960128462968233\n","accuracy :  0.5463597779954293\n","accuracy :  0.5292197192295135\n","Epoch 1, Loss: 1.9148964681139815\n","Epoch 2, Loss: 1.7215863414128207\n","Epoch 3, Loss: 1.638641298984423\n","accuracy :  0.49918380672543256\n","accuracy :  0.4763303950375449\n","Epoch 1, Loss: 1.8809449927147002\n","Epoch 2, Loss: 1.6798633861479497\n","Epoch 3, Loss: 1.6022528298685508\n","accuracy :  0.5615409729023833\n","accuracy :  0.5421155729676788\n","Epoch 1, Loss: 1.8199810234747107\n","Epoch 2, Loss: 1.6130778977049238\n","Epoch 3, Loss: 1.543283468360378\n","accuracy :  0.5555011426705844\n","accuracy :  0.5257917074763304\n","Epoch 1, Loss: 1.8462294609366137\n","Epoch 2, Loss: 1.6392422628153709\n","Epoch 3, Loss: 1.5743458601731233\n","accuracy :  0.554031994776363\n","accuracy :  0.5288932419196866\n","Epoch 1, Loss: 1.8910188206493388\n","Epoch 2, Loss: 1.672996224918191\n","Epoch 3, Loss: 1.6038578642877523\n","accuracy :  0.5714985308521058\n","accuracy :  0.5365654587006203\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        correct = 0\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            correct += np.sum(predicted == labels.cpu().numpy())\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    print('accuracy : ' , correct / len(data_loader.dataset))\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","shadow_model_count = 10\n","\n","shadow_train_outputs = []\n","shadow_train_labels = []\n","shadow_test_outputs = []\n","shadow_test_labels = []\n","\n","for _ in range(shadow_model_count):\n","    train_size = int(0.5 * len(synthetic_dataset))\n","    val_size = len(synthetic_dataset) - train_size\n","    train_dataset_shadow , test_dataset_shadow = random_split(synthetic_dataset, [train_size,val_size])\n","\n","    train_loader_shadow = DataLoader(train_dataset_shadow, batch_size = 8,shuffle = True)\n","    test_loader_shadow = DataLoader(test_dataset_shadow, batch_size = 64,shuffle = False)\n","    \n","    shadow_model = CIFAR10Classifier().to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(shadow_model.parameters(), lr=0.001, weight_decay=1e-2)\n","    \n","    train_model(shadow_model, train_loader_shadow, criterion, optimizer , 3)\n","    \n","    train_outputs, train_labels = test_model(shadow_model, train_loader_shadow)\n","    shadow_train_outputs.append(train_outputs)\n","    shadow_train_labels.append(torch.ones_like(train_labels))  # Label 1 for seen data\n","    \n","    test_outputs, test_labels = test_model(shadow_model, test_loader_shadow)\n","    shadow_test_outputs.append(test_outputs)\n","    shadow_test_labels.append(torch.zeros_like(test_labels)) # Label 0 for un-seen data\n","\n","all_train_outputs = torch.cat(shadow_train_outputs)\n","all_train_labels = torch.cat(shadow_train_labels)\n","all_test_outputs = torch.cat(shadow_test_outputs)\n","all_test_labels = torch.cat(shadow_test_labels)\n","\n","attack_inputs = torch.cat((all_train_outputs, all_test_outputs))\n","attack_labels = torch.cat((all_train_labels, all_test_labels))\n","\n","attack_features = attack_inputs.cpu().view(attack_inputs.size(0), -1).numpy()\n","attack_labels = attack_labels.cpu().numpy()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:54:41.547386Z","iopub.status.busy":"2024-07-03T06:54:41.547033Z","iopub.status.idle":"2024-07-03T06:54:55.590401Z","shell.execute_reply":"2024-07-03T06:54:55.589367Z","shell.execute_reply.started":"2024-07-03T06:54:41.547354Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Train Accuracy: 54.27%\n","Attack Model Test Accuracy: 50.05%\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","X_train, X_test, y_train, y_test = train_test_split(attack_features, attack_labels, test_size=0.2)\n","\n","attack_model = RandomForestClassifier(max_depth=6)\n","attack_model.fit(X_train, y_train)\n","\n","attack_predictions = attack_model.predict(X_train)\n","attack_accuracy = accuracy_score(y_train, attack_predictions)\n","print(f'Attack Model Train Accuracy: {attack_accuracy * 100:.2f}%')\n","\n","attack_predictions = attack_model.predict(X_test)\n","attack_accuracy = accuracy_score(y_test, attack_predictions)\n","print(f'Attack Model Test Accuracy: {attack_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:55:17.730437Z","iopub.status.busy":"2024-07-03T06:55:17.730058Z","iopub.status.idle":"2024-07-03T06:56:24.666913Z","shell.execute_reply":"2024-07-03T06:56:24.665893Z","shell.execute_reply.started":"2024-07-03T06:55:17.730407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Loss: 0.6933\n","accuracy train : 0.5008059908586353 \n","accuracy test : 0.5083251714005876 \n","Epoch [2/5], Loss: 0.6932\n","accuracy train : 0.5023771629121776 \n","accuracy test : 0.4953885079986941 \n","Epoch [3/5], Loss: 0.6932\n","accuracy train : 0.501540564805746 \n","accuracy test : 0.493837740777016 \n","Epoch [4/5], Loss: 0.6932\n","accuracy train : 0.5028974861247143 \n","accuracy test : 0.4951436500163239 \n","Epoch [5/5], Loss: 0.6931\n","accuracy train : 0.5052950538687562 \n","accuracy test : 0.5013059092393078 \n"]}],"source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","X_train, X_test, y_train, y_test = train_test_split(attack_features, attack_labels, test_size=0.2)\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        # Define layers\n","        self.fc1 = nn.Linear(4, 16)  \n","        self.fc2 = nn.Linear(16, 8) \n","        self.fc3 = nn.Linear(8, 1) \n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x)) \n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","train_attack_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train).view(-1,1).to(torch.float32))\n","train_attack_loader = DataLoader(train_attack_dataset, batch_size=16, shuffle=True)\n","\n","test_attack_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test).view(-1,1).to(torch.float32))\n","test_attack_loader = DataLoader(test_attack_dataset, batch_size=16, shuffle=False)\n","\n","attacker = MLP().to(device)\n","criterion = nn.BCELoss()\n","optimizer = optim.SGD(attacker.parameters(), lr=0.01) \n","\n","def eval_attacker(attacker, loader, text):\n","    correct = 0\n","    with torch.no_grad():\n","        for batch_data, batch_targets in loader:\n","            batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)\n","            outputs = attacker(batch_data)\n","            predicted = np.round(outputs.cpu().numpy())\n","            correct += np.sum(predicted == batch_targets.cpu().numpy())\n","    print(f'accuracy {text} : {correct / len(loader.dataset)} ')\n","\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0\n","    for batch_data, batch_targets in train_attack_loader:\n","        batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = attacker(batch_data)\n","        loss = criterion(outputs, batch_targets)\n","        running_loss += loss.item() * batch_data.size(0)\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_attack_loader.dataset):.4f}')\n","    eval_attacker(attacker, train_attack_loader, 'train')\n","    eval_attacker(attacker, test_attack_loader, 'test')\n","torch.save(attacker.state_dict(), 'attacker.pth')"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T07:26:41.331748Z","iopub.status.busy":"2024-07-03T07:26:41.331469Z","iopub.status.idle":"2024-07-03T07:26:53.697292Z","shell.execute_reply":"2024-07-03T07:26:53.696379Z","shell.execute_reply.started":"2024-07-03T07:26:41.331722Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","20000\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Training Accuracy: 0.6696\n","Confusion Matrix:\n","[[10316  9684]\n"," [ 3532 16468]]\n","Precision: 0.6297\n","Recall: 0.8234\n","F1 Score: 0.7136\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.datasets import CIFAR10\n","from torchvision import transforms\n","from torch.utils.data import Subset, DataLoader, TensorDataset\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score ,f1_score\n","from sklearn.linear_model import LogisticRegression\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = CIFAR10Classifier()\n","state_dict = torch.load(\"/kaggle/input/private-model/model_state_dict.pth\", map_location=device)\n","new_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\n","model.load_state_dict(new_state_dict)\n","model.to(device)\n","model.eval()\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","DATA_ROOT = '../cifar10'\n","BATCH_SIZE = 1024\n","\n","# Load the indices from list.txt\n","indices_file = '/kaggle/input/list-txt/list.txt' ############\n","with open(indices_file, 'r') as f:\n","    indices = [int(line.strip()) for line in f]\n","\n","full_train_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n","test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n","\n","train_indices_set = set(indices)\n","all_indices = set(range(len(full_train_dataset)))\n","other_indices = list(all_indices - train_indices_set)\n","\n","train_dataset = Subset(full_train_dataset, indices[:len(indices)//2])  ###########\n","other_dataset = Subset(full_train_dataset, other_indices)\n","print(len(train_dataset))\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","other_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# Create labels\n","train_labels = torch.ones(len(train_dataset)).to(device)\n","other_labels = torch.zeros(len(other_dataset)).to(device)\n","test_labels = torch.zeros(len(test_dataset)).to(device)\n","####################################\n","#if you have an attacker model for each class, modify the above code.\n","####################################\n","\n","def extract_features(model, dataloader):\n","    model.eval()\n","    features = []\n","    with torch.no_grad():\n","        for data in dataloader:\n","            inputs, _ = data\n","            inputs = inputs.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            outputs=-np.sort(-outputs , axis=1)\n","            features.append(torch.Tensor(outputs[:,:4]))\n","    return torch.cat(features).to(device)\n","\n","train_features = extract_features(model, train_loader)\n","other_features = extract_features(model, other_loader)\n","test_features = extract_features(model, test_loader)\n","\n","\n","combined_features = torch.cat((train_features, other_features, test_features))\n","combined_labels = torch.cat((train_labels, other_labels, test_labels))\n","\n","\n","new_dataset = TensorDataset(combined_features, combined_labels)\n","new_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","#load your attacker model\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        # Define layers\n","        self.fc1 = nn.Linear(4, 16)  \n","        self.fc2 = nn.Linear(16, 8) \n","        self.fc3 = nn.Linear(8, 1) \n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x)) \n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        x = self.sigmoid(x)\n","        return x\n","attacker = MLP().to(device)\n","attacker.load_state_dict(torch.load('/kaggle/working/attacker.pth'))\n","#############################################\n","\n","# Calculate training accuracy, confusion matrix, precision, and recall\n","attacker.eval()\n","all_labels = []\n","all_predicted = []\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for features, labels in new_loader:\n","        features, labels = features.to(device), labels.to(device)\n","        outputs = attacker(features).squeeze()\n","        predicted = (outputs > 0.5).float()\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        all_labels.extend(labels.cpu().numpy())\n","        all_predicted.extend(predicted.cpu().numpy())\n","\n","accuracy = correct / total\n","print(f'Training Accuracy: {accuracy:.4f}')\n","\n","cm = confusion_matrix(all_labels, all_predicted)\n","precision = precision_score(all_labels, all_predicted)\n","recall = recall_score(all_labels, all_predicted)\n","f1 = f1_score(all_labels, all_predicted)\n","\n","print(f'Confusion Matrix:\\n{cm}')\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5324273,"sourceId":8846058,"sourceType":"datasetVersion"},{"datasetId":5324283,"sourceId":8846072,"sourceType":"datasetVersion"},{"datasetId":5324570,"sourceId":8846471,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
