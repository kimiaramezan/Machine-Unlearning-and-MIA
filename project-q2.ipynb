{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-02T05:51:55.022907Z","iopub.status.busy":"2024-07-02T05:51:55.022560Z","iopub.status.idle":"2024-07-02T05:52:01.658513Z","shell.execute_reply":"2024-07-02T05:52:01.657670Z","shell.execute_reply.started":"2024-07-02T05:51:55.022878Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from torch.utils.data import DataLoader, random_split, Subset\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Subset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.optim as optim\n","import torch.nn as nn\n","import numpy as np\n","\n","class CIFAR10Classifier(nn.Module):\n","    def __init__(self):\n","        super(CIFAR10Classifier, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n","        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        self.fc1 = nn.Linear(6272, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:53:06.566488Z","iopub.status.busy":"2024-07-02T05:53:06.565736Z","iopub.status.idle":"2024-07-02T05:53:08.116160Z","shell.execute_reply":"2024-07-02T05:53:08.115363Z","shell.execute_reply.started":"2024-07-02T05:53:06.566454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","split = int(np.floor(0.2 * num_train))\n","train_idx, test_idx = indices[split:], indices[:split]\n","\n","train_dataset = Subset(trainset,train_idx)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n","test_testset = torch.utils.data.ConcatDataset([Subset(trainset, test_idx), testset])\n","test_loader = torch.utils.data.DataLoader(test_testset, batch_size=64, shuffle=False)\n","\n","def evaluate(model):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Train Accuracy: {100 * correct / total}%')\n","\n","    # Test loop\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Test Accuracy: {100 * correct / total}%')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T18:41:57.737064Z","iopub.status.busy":"2024-06-27T18:41:57.736645Z","iopub.status.idle":"2024-06-27T18:51:44.263032Z","shell.execute_reply":"2024-06-27T18:51:44.261888Z","shell.execute_reply.started":"2024-06-27T18:41:57.737029Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1, Loss: 0.6031436642050743\n","Epoch 2, Loss: 0.5934444001317024\n","Epoch 3, Loss: 0.5966733865857125\n","Epoch 4, Loss: 0.5896390744328499\n","Epoch 5, Loss: 0.595965623998642\n","Train Accuracy: 97.5475%\n","Test Accuracy: 64.575%\n","Epoch 6, Loss: 0.5916860419511795\n","Epoch 7, Loss: 0.6009456028938294\n","Epoch 00008: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 8, Loss: 0.5938847616076469\n","Epoch 9, Loss: 0.5898481168746948\n","Epoch 10, Loss: 0.5927182900071144\n","Train Accuracy: 97.68%\n","Test Accuracy: 64.75%\n","Epoch 11, Loss: 0.5884663327217102\n","Epoch 12, Loss: 0.5938656493544578\n","Epoch 13, Loss: 0.5887955960392952\n","Epoch 14, Loss: 0.579175396323204\n","Epoch 15, Loss: 0.5845593259334564\n","Train Accuracy: 97.7325%\n","Test Accuracy: 64.825%\n","Epoch 16, Loss: 0.5897984580874444\n","Epoch 17, Loss: 0.5943819497942925\n","Epoch 00018: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 18, Loss: 0.5830100085377693\n","Epoch 19, Loss: 0.5851366110801697\n","Epoch 20, Loss: 0.5790835438132286\n","Train Accuracy: 97.71%\n","Test Accuracy: 64.88%\n","Epoch 21, Loss: 0.5866350921988487\n","Epoch 22, Loss: 0.584448201572895\n","Epoch 23, Loss: 0.5857028846144676\n","Epoch 00024: reducing learning rate of group 0 to 6.2500e-06.\n","Epoch 24, Loss: 0.5893513982772827\n","Epoch 25, Loss: 0.5791132676243782\n","Train Accuracy: 97.78%\n","Test Accuracy: 64.84%\n","Epoch 26, Loss: 0.5851686410665512\n","Epoch 27, Loss: 0.5912631803274154\n","Epoch 00028: reducing learning rate of group 0 to 3.1250e-06.\n","Epoch 28, Loss: 0.5804139006853104\n","Epoch 29, Loss: 0.5847522315979004\n","Epoch 30, Loss: 0.5777776260375976\n","Train Accuracy: 97.7775%\n","Test Accuracy: 64.865%\n"]}],"source":["n_epochs = 30\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()*inputs.size(0)\n","    epoch_loss=running_loss/len(train_loader.dataset)\n","    scheduler.step(epoch_loss)\n","    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n","    if epoch % 5 == 4:\n","        evaluate(model)     \n","torch.save(model.state_dict(),'model.pth')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-28T06:08:37.136738Z","iopub.status.busy":"2024-06-28T06:08:37.136008Z","iopub.status.idle":"2024-06-28T06:18:33.405089Z","shell.execute_reply":"2024-06-28T06:18:33.404118Z","shell.execute_reply.started":"2024-06-28T06:08:37.136707Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.6004154583573341\n","Epoch 2, Loss: 0.6423797505378723\n","Epoch 3, Loss: 0.6976489079475403\n","Epoch 4, Loss: 0.7635789583444595\n","Epoch 5, Loss: 0.8239570449352265\n","Train Accuracy: 87.84%\n","Test Accuracy: 66.885%\n","Epoch 6, Loss: 0.4890729764461517\n","Epoch 7, Loss: 0.4975093710899353\n","Epoch 8, Loss: 0.5104012539625168\n","Epoch 9, Loss: 0.5174827815175056\n","Epoch 10, Loss: 0.519823055934906\n","Train Accuracy: 85.085%\n","Test Accuracy: 68.925%\n","Epoch 11, Loss: 0.5035010778427124\n","Epoch 12, Loss: 0.5004420328378677\n","Epoch 13, Loss: 0.49729390136003493\n","Epoch 14, Loss: 0.49383416311740874\n","Epoch 15, Loss: 0.4899659893989563\n","Train Accuracy: 86.1675%\n","Test Accuracy: 69.325%\n","Epoch 16, Loss: 0.4778000099658966\n","Epoch 17, Loss: 0.4750052743673325\n","Epoch 18, Loss: 0.47233768293857575\n","Epoch 19, Loss: 0.4696965965151787\n","Epoch 20, Loss: 0.4670365028858185\n","Train Accuracy: 86.925%\n","Test Accuracy: 69.365%\n","Epoch 21, Loss: 0.4603620710134506\n","Epoch 22, Loss: 0.4587628855228424\n","Epoch 23, Loss: 0.45722616648674014\n","Epoch 24, Loss: 0.4557299329519272\n","Epoch 25, Loss: 0.45421923323869706\n","Train Accuracy: 87.335%\n","Test Accuracy: 69.39%\n","Epoch 26, Loss: 0.45081525816917417\n","Epoch 27, Loss: 0.4499509576797485\n","Epoch 28, Loss: 0.4491234711408615\n","Epoch 29, Loss: 0.44831785764694215\n","Epoch 30, Loss: 0.44751621428728106\n","Train Accuracy: 87.6725%\n","Test Accuracy: 69.42%\n","Epoch 31, Loss: 0.4457439467430115\n","Epoch 32, Loss: 0.44526947548389434\n","Epoch 33, Loss: 0.44485152858495713\n","Epoch 34, Loss: 0.44443449007272723\n","Epoch 35, Loss: 0.44402475563287735\n","Train Accuracy: 87.805%\n","Test Accuracy: 69.455%\n","Epoch 36, Loss: 0.44309061192274096\n","Epoch 37, Loss: 0.44283013690710066\n","Epoch 38, Loss: 0.4426144338965416\n","Epoch 39, Loss: 0.44240433608293533\n","Epoch 40, Loss: 0.4421957946062088\n","Train Accuracy: 87.8675%\n","Test Accuracy: 69.52%\n"]}],"source":["model_privacy = CIFAR10Classifier().to(device)\n","model_privacy.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model_privacy.parameters(), lr=0.0001,weight_decay=1e-2)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)\n","\n","n_epochs = 40\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model_privacy(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()*inputs.size(0)\n","    epoch_loss=running_loss/len(train_loader.dataset)\n","    scheduler.step()\n","    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n","    if epoch % 5 == 4:\n","        evaluate(model_privacy)\n","torch.save(model_privacy.state_dict(),'model_privacy_reg.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-28T06:34:20.506498Z","iopub.status.busy":"2024-06-28T06:34:20.505653Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.4049977184295654\n","Epoch 2, Loss: 1.2372498091697692\n","Epoch 3, Loss: 1.188036063671112\n","Epoch 4, Loss: 1.1483295279502868\n","Epoch 5, Loss: 1.1272585335731506\n","Train Accuracy: 87.635%\n","Test Accuracy: 59.49%\n","Epoch 6, Loss: 0.5784820919513702\n","Epoch 7, Loss: 0.500895554447174\n","Epoch 8, Loss: 0.46153420984745025\n","Epoch 9, Loss: 0.42940368609428403\n","Epoch 10, Loss: 0.40040717749595645\n","Train Accuracy: 86.935%\n","Test Accuracy: 60.265%\n","Epoch 11, Loss: 0.37296193571090697\n","Epoch 12, Loss: 0.34815986838340757\n","Epoch 13, Loss: 0.32405457615852357\n","Epoch 14, Loss: 0.30117551758289335\n","Epoch 15, Loss: 0.27965687630176544\n","Train Accuracy: 87.73%\n","Test Accuracy: 60.285%\n","Epoch 16, Loss: 0.25921883091926573\n","Epoch 17, Loss: 0.23980494067668914\n","Epoch 18, Loss: 0.22122882915139197\n","Epoch 19, Loss: 0.20337558255791663\n","Epoch 20, Loss: 0.18646880429387092\n","Train Accuracy: 87.8925%\n","Test Accuracy: 60.12%\n","Epoch 21, Loss: 0.1705017638117075\n","Epoch 22, Loss: 0.15522121312618256\n","Epoch 23, Loss: 0.1407543937921524\n","Epoch 24, Loss: 0.12725546668171883\n","Epoch 25, Loss: 0.11467881318032741\n","Train Accuracy: 88.2675%\n","Test Accuracy: 60.21%\n","Epoch 26, Loss: 0.10290415933728218\n","Epoch 27, Loss: 0.09183320895135402\n","Epoch 28, Loss: 0.08173116118311882\n","Epoch 29, Loss: 0.07221456106901168\n","Epoch 30, Loss: 0.06377320468276738\n"]}],"source":["poisoned_images = []\n","poisoned_labels = []\n","\n","for i in range(len(train_dataset)):\n","    image, label = train_dataset[i]\n","    image_copy = image.clone()\n","    \n","    std_noise = 0.3\n","    noise = np.random.normal(size=(3, 32, 32)) * std_noise\n","    image_copy += noise\n","    poisoned_images.append(image_copy)\n","    poisoned_labels.append(label)\n","    \n","poisoned_images = torch.stack(poisoned_images).to(torch.float32)\n","poisoned_labels = torch.tensor(poisoned_labels)\n","\n","class NoisyDataset(torch.utils.data.Dataset):\n","    def __init__(self, images, labels):\n","        self.images = images\n","        self.labels = labels\n","    \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        return self.images[idx], self.labels[idx]\n","    \n","noisy_dataset = NoisyDataset(poisoned_images, poisoned_labels)\n","noisy_loader = torch.utils.data.DataLoader(noisy_dataset, batch_size = 64, shuffle=True)\n","\n","model_privacy = CIFAR10Classifier().to(device)\n","model_privacy.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model_privacy.parameters(), lr=0.0001)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n","\n","n_epochs = 40\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in noisy_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model_privacy(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()*inputs.size(0)\n","    epoch_loss=running_loss/len(noisy_loader.dataset)\n","    scheduler.step(epoch_loss)\n","    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n","    if epoch % 5 == 4:\n","        evaluate(model_privacy)\n","torch.save(model_privacy.state_dict(),'model_privacy_noise.pth')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:33:03.749555Z","iopub.status.busy":"2024-07-02T06:33:03.748882Z","iopub.status.idle":"2024-07-02T06:33:32.719287Z","shell.execute_reply":"2024-07-02T06:33:32.718260Z","shell.execute_reply.started":"2024-07-02T06:33:03.749521Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 1.3207589384085072\n","Epoch 10, Loss: 1.1258211568140755\n","Epoch 15, Loss: 0.9886573764462822\n","Epoch 20, Loss: 0.8786869955519898\n","accuracy :  0.8703\n","accuracy :  0.6152\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        scheduler.step()\n","        if epoch % 5 == 4:\n","            print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        correct = 0\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            correct += np.sum(predicted == labels.cpu().numpy())\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    print('accuracy : ' , correct / len(data_loader.dataset))\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","shadow_model_count = 1\n","\n","shadow_train_outputs = []\n","shadow_train_labels = []\n","shadow_test_outputs = []\n","shadow_test_labels = []\n","\n","for _ in range(shadow_model_count):\n","    shadow_train_data, shadow_test_data = train_test_split(train_dataset, test_size=0.5)\n","    shadow_train_loader = DataLoader(shadow_train_data, batch_size=64, shuffle=False)\n","    shadow_test_loader = DataLoader(shadow_test_data, batch_size=64, shuffle=False)\n","    \n","    model = CIFAR10Classifier().to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=20 , gamma = 0.7)\n","    train_model(model, shadow_train_loader, criterion, optimizer, scheduler , 20)\n","    \n","    train_outputs, train_labels = test_model(model, shadow_train_loader)\n","    shadow_train_outputs.append(train_outputs)\n","    shadow_train_labels.append(torch.ones_like(train_labels))  # Label 1 for seen data\n","    \n","    test_outputs, test_labels = test_model(model, shadow_test_loader)\n","    shadow_test_outputs.append(test_outputs)\n","    shadow_test_labels.append(torch.zeros_like(test_labels)) # Label 0 for un-seen data\n","\n","all_train_outputs = torch.cat(shadow_train_outputs)\n","all_train_labels = torch.cat(shadow_train_labels)\n","all_test_outputs = torch.cat(shadow_test_outputs)\n","all_test_labels = torch.cat(shadow_test_labels)\n","\n","attack_inputs = torch.cat((all_train_outputs, all_test_outputs))\n","attack_labels = torch.cat((all_train_labels, all_test_labels))\n","\n","attack_features = attack_inputs.cpu().view(attack_inputs.size(0), -1).numpy()\n","attack_labels = attack_labels.cpu().numpy()\n","\n","X_train, X_test, y_train, y_test = train_test_split(attack_features, attack_labels, test_size=0.2)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:33:58.455383Z","iopub.status.busy":"2024-07-02T06:33:58.455000Z","iopub.status.idle":"2024-07-02T06:34:02.810147Z","shell.execute_reply":"2024-07-02T06:34:02.809106Z","shell.execute_reply.started":"2024-07-02T06:33:58.455348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Train Accuracy: 59.37%\n","Attack Model Test Accuracy: 56.49%\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","attack_model = RandomForestClassifier(max_depth=7)\n","attack_model.fit(X_train, y_train)\n","\n","attack_predictions = attack_model.predict(X_train)\n","attack_accuracy = accuracy_score(y_train, attack_predictions)\n","print(f'Attack Model Train Accuracy: {attack_accuracy * 100:.2f}%')\n","\n","attack_predictions = attack_model.predict(X_test)\n","attack_accuracy = accuracy_score(y_test, attack_predictions)\n","print(f'Attack Model Test Accuracy: {attack_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:35:55.532641Z","iopub.status.busy":"2024-07-02T06:35:55.531531Z","iopub.status.idle":"2024-07-02T06:36:17.296378Z","shell.execute_reply":"2024-07-02T06:36:17.295282Z","shell.execute_reply.started":"2024-07-02T06:35:55.532608Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["baseline model : \n","privacy noise model\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Baseline Model MIA Predictions: 0.58215\n","privacy noise Model MIA Predictions: 0.52555\n"]}],"source":["def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","def evaluate_mia(model, attack_model, data_loader):\n","    model_outputs, _ = test_model(model, data_loader)\n","    model_features = model_outputs.cpu().view(model_outputs.size(0), -1).numpy()\n","    mia_predictions = attack_model.predict(model_features)\n","    labels = np.zeros(40000)\n","    labels[20000:] = 1\n","    return accuracy_score(mia_predictions, labels)\n","\n","baseline_model = CIFAR10Classifier().to(device)\n","baseline_model.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","print('baseline model : ')\n","\n","privacy_model_noise = CIFAR10Classifier().to(device)\n","privacy_model_noise.load_state_dict(torch.load('/kaggle/input/my-models/model_privacy_noise.pth'))\n","print('privacy noise model')\n","\n","evaluate_MIA_dataset = torch.utils.data.ConcatDataset([test_testset, Subset(train_dataset,np.arange(len(test_testset)))])\n","evaluate_MIA_loader = DataLoader(evaluate_MIA_dataset, batch_size = 64 , shuffle = False) \n","\n","baseline_model_predictions = evaluate_mia(baseline_model, attack_model, evaluate_MIA_loader)\n","privacy_noise_model_predictions = evaluate_mia(privacy_model_noise, attack_model, evaluate_MIA_loader)\n","\n","print(f'Baseline Model MIA Predictions: {baseline_model_predictions}')\n","print(f'privacy noise Model MIA Predictions: {privacy_noise_model_predictions}')"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:44:30.326365Z","iopub.status.busy":"2024-07-02T06:44:30.326002Z","iopub.status.idle":"2024-07-02T06:49:34.778182Z","shell.execute_reply":"2024-07-02T06:49:34.777143Z","shell.execute_reply.started":"2024-07-02T06:44:30.326335Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 1.3088257981184572\n","Epoch 10, Loss: 1.1003151004687666\n","Epoch 15, Loss: 0.9573293552992824\n","Epoch 20, Loss: 0.8802372241934268\n","Epoch 25, Loss: 0.772881128917487\n","Epoch 30, Loss: 0.7408470376231038\n","Epoch 35, Loss: 0.6768344717856032\n","Epoch 40, Loss: 0.6614648207498435\n","Epoch 45, Loss: 0.6254129116527569\n","Epoch 50, Loss: 0.6030874855982991\n","Epoch 55, Loss: 0.5900167959947555\n","Epoch 60, Loss: 0.571661868796181\n","Epoch 65, Loss: 0.5435910688135952\n","Epoch 70, Loss: 0.548027363067237\n","Epoch 75, Loss: 0.5298273665265153\n","Epoch 80, Loss: 0.5296404283648481\n","Epoch 85, Loss: 0.5161730939397415\n","Epoch 90, Loss: 0.5144668611855553\n","Epoch 95, Loss: 0.5091935128163987\n","Epoch 100, Loss: 0.5069299711586949\n","accuracy :  0.9903\n","accuracy :  0.61655\n","Epoch 5, Loss: 1.2936482694202338\n","Epoch 10, Loss: 1.0889426456472744\n","Epoch 15, Loss: 0.9473179498800455\n","Epoch 20, Loss: 0.85269438706267\n","Epoch 25, Loss: 0.7640634867520378\n","Epoch 30, Loss: 0.7081203762525187\n","Epoch 35, Loss: 0.6616898259034933\n","Epoch 40, Loss: 0.6483180310589056\n","Epoch 45, Loss: 0.6030042784663435\n","Epoch 50, Loss: 0.5810582047453323\n","Epoch 55, Loss: 0.5579797406546986\n","Epoch 60, Loss: 0.5528405238264285\n","Epoch 65, Loss: 0.5471441739093\n","Epoch 70, Loss: 0.5222548113082545\n","Epoch 75, Loss: 0.5151653815382204\n","Epoch 80, Loss: 0.5044160190576943\n","Epoch 85, Loss: 0.5162392581423251\n","Epoch 90, Loss: 0.4976052251962808\n","Epoch 95, Loss: 0.4972061250156488\n","Epoch 100, Loss: 0.4869266768423513\n","accuracy :  0.9901\n","accuracy :  0.62065\n","Epoch 5, Loss: 1.3831047208164446\n","Epoch 10, Loss: 1.1588372460569436\n","Epoch 15, Loss: 1.0045089186570895\n","Epoch 20, Loss: 0.9216732883605713\n","Epoch 25, Loss: 0.8389449468055091\n","Epoch 30, Loss: 0.7892575760047656\n","Epoch 35, Loss: 0.7230829058554226\n","Epoch 40, Loss: 0.6939810852463634\n","Epoch 45, Loss: 0.6666300935676684\n","Epoch 50, Loss: 0.6420171632172581\n","Epoch 55, Loss: 0.6265111993105648\n","Epoch 60, Loss: 0.6012781183845319\n","Epoch 65, Loss: 0.5847522040335135\n","Epoch 70, Loss: 0.5864451623762759\n","Epoch 75, Loss: 0.5576340255265038\n","Epoch 80, Loss: 0.567321202720697\n","Epoch 85, Loss: 0.5559641578422186\n","Epoch 90, Loss: 0.5417853496706905\n","Epoch 95, Loss: 0.5427691767962215\n","Epoch 100, Loss: 0.5357617931053661\n","accuracy :  0.98525\n","accuracy :  0.62095\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        scheduler.step()\n","        if epoch % 5 == 4:\n","            print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        correct = 0\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            correct += np.sum(predicted == labels.cpu().numpy())\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    print('accuracy : ' , correct / len(data_loader.dataset))\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","shadow_model_count = 3\n","\n","shadow_train_outputs = []\n","shadow_train_labels = []\n","shadow_test_outputs = []\n","shadow_test_labels = []\n","\n","for _ in range(shadow_model_count):\n","    shadow_train_data, shadow_test_data = train_test_split(train_dataset, test_size=0.5)\n","    shadow_train_loader = DataLoader(shadow_train_data, batch_size=64, shuffle=False)\n","    shadow_test_loader = DataLoader(shadow_test_data, batch_size=64, shuffle=False)\n","    \n","    model = CIFAR10Classifier().to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=10 , gamma = 0.7)\n","    train_model(model, shadow_train_loader, criterion, optimizer, scheduler , 100)\n","    \n","    train_outputs, train_labels = test_model(model, shadow_train_loader)\n","    shadow_train_outputs.append(train_outputs)\n","    shadow_train_labels.append(torch.ones_like(train_labels))  # Label 1 for seen data\n","    \n","    test_outputs, test_labels = test_model(model, shadow_test_loader)\n","    shadow_test_outputs.append(test_outputs)\n","    shadow_test_labels.append(torch.zeros_like(test_labels)) # Label 0 for un-seen data\n","\n","all_train_outputs = torch.cat(shadow_train_outputs)\n","all_train_labels = torch.cat(shadow_train_labels)\n","all_test_outputs = torch.cat(shadow_test_outputs)\n","all_test_labels = torch.cat(shadow_test_labels)\n","\n","attack_inputs = torch.cat((all_train_outputs, all_test_outputs))\n","attack_labels = torch.cat((all_train_labels, all_test_labels))\n","\n","attack_features = attack_inputs.cpu().view(attack_inputs.size(0), -1).numpy()\n","attack_labels = attack_labels.cpu().numpy()\n","\n","X_train, X_test, y_train, y_test = train_test_split(attack_features, attack_labels, test_size=0.2)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:51:04.621953Z","iopub.status.busy":"2024-07-02T06:51:04.621101Z","iopub.status.idle":"2024-07-02T06:51:18.788388Z","shell.execute_reply":"2024-07-02T06:51:18.787429Z","shell.execute_reply.started":"2024-07-02T06:51:04.621916Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Train Accuracy: 67.66%\n","Attack Model Test Accuracy: 67.18%\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","attack_model = RandomForestClassifier(max_depth=7)\n","attack_model.fit(X_train, y_train)\n","\n","attack_predictions = attack_model.predict(X_train)\n","attack_accuracy = accuracy_score(y_train, attack_predictions)\n","print(f'Attack Model Train Accuracy: {attack_accuracy * 100:.2f}%')\n","\n","attack_predictions = attack_model.predict(X_test)\n","attack_accuracy = accuracy_score(y_test, attack_predictions)\n","print(f'Attack Model Test Accuracy: {attack_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:51:32.512623Z","iopub.status.busy":"2024-07-02T06:51:32.512245Z","iopub.status.idle":"2024-07-02T06:51:54.180438Z","shell.execute_reply":"2024-07-02T06:51:54.179526Z","shell.execute_reply.started":"2024-07-02T06:51:32.512593Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["baseline model : \n","privacy noise model\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n","/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Baseline Model MIA Predictions: 0.654238\n","privacy noise Model MIA Predictions: 0.537625\n"]}],"source":["def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","def evaluate_mia(model, attack_model, data_loader):\n","    model_outputs, _ = test_model(model, data_loader)\n","    model_features = model_outputs.cpu().view(model_outputs.size(0), -1).numpy()\n","    mia_predictions = attack_model.predict(model_features)\n","    labels = np.zeros(40000)\n","    labels[20000:] = 1\n","    return accuracy_score(mia_predictions, labels)\n","\n","baseline_model = CIFAR10Classifier().to(device)\n","baseline_model.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","print('baseline model : ')\n","\n","privacy_model_noise = CIFAR10Classifier().to(device)\n","privacy_model_noise.load_state_dict(torch.load('/kaggle/input/my-models/model_privacy_noise.pth'))\n","print('privacy noise model')\n","\n","evaluate_MIA_dataset = torch.utils.data.ConcatDataset([test_testset, Subset(train_dataset,np.arange(len(test_testset)))])\n","evaluate_MIA_loader = DataLoader(evaluate_MIA_dataset, batch_size = 64 , shuffle = False) \n","\n","baseline_model_predictions = evaluate_mia(baseline_model, attack_model, evaluate_MIA_loader)\n","privacy_noise_model_predictions = evaluate_mia(privacy_model_noise, attack_model, evaluate_MIA_loader)\n","\n","print(f'Baseline Model MIA Predictions: {baseline_model_predictions}')\n","print(f'privacy noise Model MIA Predictions: {privacy_noise_model_predictions}')"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:52:45.287990Z","iopub.status.busy":"2024-07-02T06:52:45.287354Z","iopub.status.idle":"2024-07-02T06:53:41.919421Z","shell.execute_reply":"2024-07-02T06:53:41.918465Z","shell.execute_reply.started":"2024-07-02T06:52:45.287955Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 1.541419640897562\n","Epoch 10, Loss: 1.4384916339057703\n","Epoch 15, Loss: 1.3240397166901123\n","Epoch 20, Loss: 1.2794586595254964\n","Epoch 25, Loss: 1.2065467299363863\n","Epoch 30, Loss: 1.1798430418434995\n","Epoch 35, Loss: 1.1134689861593154\n","Epoch 40, Loss: 1.1038193043809348\n","Epoch 45, Loss: 1.0648910702227024\n","Epoch 50, Loss: 1.0564359774985634\n","accuracy :  0.7603\n","accuracy :  0.63615\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        scheduler.step()\n","        if epoch % 5 == 4:\n","            print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        correct = 0\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            correct += np.sum(predicted == labels.cpu().numpy())\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    print('accuracy : ' , correct / len(data_loader.dataset))\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","shadow_model_count = 1\n","\n","shadow_train_outputs = []\n","shadow_train_labels = []\n","shadow_test_outputs = []\n","shadow_test_labels = []\n","\n","for _ in range(shadow_model_count):\n","    shadow_train_data, shadow_test_data = train_test_split(train_dataset, test_size=0.5)\n","    shadow_train_loader = DataLoader(shadow_train_data, batch_size=64, shuffle=False)\n","    shadow_test_loader = DataLoader(shadow_test_data, batch_size=64, shuffle=False)\n","    \n","    model = CIFAR10Classifier().to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=10 , gamma = 0.5)\n","    train_model(model, shadow_train_loader, criterion, optimizer, scheduler , 50)\n","    \n","    train_outputs, train_labels = test_model(model, shadow_train_loader)\n","    shadow_train_outputs.append(train_outputs)\n","    shadow_train_labels.append(torch.ones_like(train_labels))  # Label 1 for seen data\n","    \n","    test_outputs, test_labels = test_model(model, shadow_test_loader)\n","    shadow_test_outputs.append(test_outputs)\n","    shadow_test_labels.append(torch.zeros_like(test_labels)) # Label 0 for un-seen data\n","\n","all_train_outputs = torch.cat(shadow_train_outputs)\n","all_train_labels = torch.cat(shadow_train_labels)\n","all_test_outputs = torch.cat(shadow_test_outputs)\n","all_test_labels = torch.cat(shadow_test_labels)\n","\n","attack_inputs = torch.cat((all_train_outputs, all_test_outputs))\n","attack_labels = torch.cat((all_train_labels, all_test_labels))\n","\n","attack_features = attack_inputs.cpu().view(attack_inputs.size(0), -1).numpy()\n","attack_labels = attack_labels.cpu().numpy()\n","\n","X_train, X_test, y_train, y_test = train_test_split(attack_features, attack_labels, test_size=0.2)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:54:47.790484Z","iopub.status.busy":"2024-07-02T06:54:47.789610Z","iopub.status.idle":"2024-07-02T06:54:52.206756Z","shell.execute_reply":"2024-07-02T06:54:52.205750Z","shell.execute_reply.started":"2024-07-02T06:54:47.790449Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Train Accuracy: 57.92%\n","Attack Model Test Accuracy: 52.00%\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","attack_model = RandomForestClassifier(max_depth=7)\n","attack_model.fit(X_train, y_train)\n","\n","attack_predictions = attack_model.predict(X_train)\n","attack_accuracy = accuracy_score(y_train, attack_predictions)\n","print(f'Attack Model Train Accuracy: {attack_accuracy * 100:.2f}%')\n","\n","attack_predictions = attack_model.predict(X_test)\n","attack_accuracy = accuracy_score(y_test, attack_predictions)\n","print(f'Attack Model Test Accuracy: {attack_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:54:57.639407Z","iopub.status.busy":"2024-07-02T06:54:57.638695Z","iopub.status.idle":"2024-07-02T06:55:08.238094Z","shell.execute_reply":"2024-07-02T06:55:08.237132Z","shell.execute_reply.started":"2024-07-02T06:54:57.639373Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["privacy regulaztion model\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["privacy regulaztion MIA Predictions: 0.53105\n"]}],"source":["def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","def evaluate_mia(model, attack_model, data_loader):\n","    model_outputs, _ = test_model(model, data_loader)\n","    model_features = model_outputs.cpu().view(model_outputs.size(0), -1).numpy()\n","    mia_predictions = attack_model.predict(model_features)\n","    labels = np.zeros(40000)\n","    labels[20000:] = 1\n","    return accuracy_score(mia_predictions, labels)\n","\n","privacy_model_reg = CIFAR10Classifier().to(device)\n","privacy_model_reg.load_state_dict(torch.load('/kaggle/input/my-models/model_privacy_reg.pth'))\n","print('privacy regulaztion model')\n","\n","evaluate_MIA_dataset = torch.utils.data.ConcatDataset([test_testset, Subset(train_dataset,np.arange(len(test_testset)))])\n","evaluate_MIA_loader = DataLoader(evaluate_MIA_dataset, batch_size = 64 , shuffle = False) \n","\n","privacy_reg_model_predictions = evaluate_mia(privacy_model_reg, attack_model, evaluate_MIA_loader)\n","\n","print(f'privacy regulaztion MIA Predictions: {privacy_reg_model_predictions}')"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:55:17.825862Z","iopub.status.busy":"2024-07-02T06:55:17.824932Z","iopub.status.idle":"2024-07-02T07:00:24.308373Z","shell.execute_reply":"2024-07-02T07:00:24.306991Z","shell.execute_reply.started":"2024-07-02T06:55:17.825809Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 1.2240301271597036\n","Epoch 10, Loss: 1.2689890808190782\n","Epoch 15, Loss: 1.1690075408917266\n","Epoch 20, Loss: 1.1580383145390227\n","Epoch 25, Loss: 1.0727429812708602\n","Epoch 30, Loss: 1.0396648833926874\n","Epoch 35, Loss: 0.9902286259130167\n","Epoch 40, Loss: 0.9648639546415676\n","Epoch 45, Loss: 0.9394692967113215\n","Epoch 50, Loss: 0.9223812809005713\n","Epoch 55, Loss: 0.9043535734898747\n","Epoch 60, Loss: 0.8915775937203783\n","Epoch 65, Loss: 0.8828917288551696\n","Epoch 70, Loss: 0.8814729686362294\n","Epoch 75, Loss: 0.8817661078974081\n","Epoch 80, Loss: 0.8741796477534138\n","Epoch 85, Loss: 0.8681775763773689\n","Epoch 90, Loss: 0.8705550463626179\n","Epoch 95, Loss: 0.8677837256425486\n","Epoch 100, Loss: 0.871726212029259\n","accuracy :  0.84365\n","accuracy :  0.6558\n","Epoch 5, Loss: 1.2253564398128765\n","Epoch 10, Loss: 1.2800788271922272\n","Epoch 15, Loss: 1.18503004045913\n","Epoch 20, Loss: 1.171807865746105\n","Epoch 25, Loss: 1.0850597968497597\n","Epoch 30, Loss: 1.069923601973171\n","Epoch 35, Loss: 1.0096930076900763\n","Epoch 40, Loss: 0.9834423148974824\n","Epoch 45, Loss: 0.9577367313372822\n","Epoch 50, Loss: 0.9365291448827749\n","Epoch 55, Loss: 0.918475795477724\n","Epoch 60, Loss: 0.9044380311767894\n","Epoch 65, Loss: 0.8943839259802724\n","Epoch 70, Loss: 0.8898158071520991\n","Epoch 75, Loss: 0.8780715728339296\n","Epoch 80, Loss: 0.8865814904054514\n","Epoch 85, Loss: 0.8904933470506637\n","Epoch 90, Loss: 0.8850564339671272\n","Epoch 95, Loss: 0.8811136604117128\n","Epoch 100, Loss: 0.8725251850609581\n","accuracy :  0.8409\n","accuracy :  0.6568\n","Epoch 5, Loss: 1.218515579883283\n","Epoch 10, Loss: 1.2672512190410505\n","Epoch 15, Loss: 1.1756180161104415\n","Epoch 20, Loss: 1.1524608264715908\n","Epoch 25, Loss: 1.067477577410567\n","Epoch 30, Loss: 1.0446692295729543\n","Epoch 35, Loss: 0.9797818030412204\n","Epoch 40, Loss: 0.9714108852151865\n","Epoch 45, Loss: 0.9189069074944566\n","Epoch 50, Loss: 0.9118522533212607\n","Epoch 55, Loss: 0.8934933177579325\n","Epoch 60, Loss: 0.8809357266456556\n","Epoch 65, Loss: 0.8720870046569897\n","Epoch 70, Loss: 0.8711269909200577\n","Epoch 75, Loss: 0.8603829423459574\n","Epoch 80, Loss: 0.8554235201674147\n","Epoch 85, Loss: 0.8582175118854632\n","Epoch 90, Loss: 0.8537175988617797\n","Epoch 95, Loss: 0.8567325569951115\n","Epoch 100, Loss: 0.8566967178457461\n","accuracy :  0.8483\n","accuracy :  0.65605\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        scheduler.step()\n","        if epoch % 5 == 4:\n","            print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        correct = 0\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            correct += np.sum(predicted == labels.cpu().numpy())\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    print('accuracy : ' , correct / len(data_loader.dataset))\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","shadow_model_count = 3\n","\n","shadow_train_outputs = []\n","shadow_train_labels = []\n","shadow_test_outputs = []\n","shadow_test_labels = []\n","\n","for _ in range(shadow_model_count):\n","    shadow_train_data, shadow_test_data = train_test_split(train_dataset, test_size=0.5)\n","    shadow_train_loader = DataLoader(shadow_train_data, batch_size=64, shuffle=False)\n","    shadow_test_loader = DataLoader(shadow_test_data, batch_size=64, shuffle=False)\n","    \n","    model = CIFAR10Classifier().to(device)\n","    model.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=10 , gamma = 0.5)\n","    train_model(model, shadow_train_loader, criterion, optimizer, scheduler , 100)\n","    \n","    train_outputs, train_labels = test_model(model, shadow_train_loader)\n","    shadow_train_outputs.append(train_outputs)\n","    shadow_train_labels.append(torch.ones_like(train_labels))  # Label 1 for seen data\n","    \n","    test_outputs, test_labels = test_model(model, shadow_test_loader)\n","    shadow_test_outputs.append(test_outputs)\n","    shadow_test_labels.append(torch.zeros_like(test_labels)) # Label 0 for un-seen data\n","\n","all_train_outputs = torch.cat(shadow_train_outputs)\n","all_train_labels = torch.cat(shadow_train_labels)\n","all_test_outputs = torch.cat(shadow_test_outputs)\n","all_test_labels = torch.cat(shadow_test_labels)\n","\n","attack_inputs = torch.cat((all_train_outputs, all_test_outputs))\n","attack_labels = torch.cat((all_train_labels, all_test_labels))\n","\n","attack_features = attack_inputs.cpu().view(attack_inputs.size(0), -1).numpy()\n","attack_labels = attack_labels.cpu().numpy()\n","\n","X_train, X_test, y_train, y_test = train_test_split(attack_features, attack_labels, test_size=0.2)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T07:00:24.310429Z","iopub.status.busy":"2024-07-02T07:00:24.310165Z","iopub.status.idle":"2024-07-02T07:00:39.149111Z","shell.execute_reply":"2024-07-02T07:00:39.148192Z","shell.execute_reply.started":"2024-07-02T07:00:24.310405Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Train Accuracy: 55.86%\n","Attack Model Test Accuracy: 55.83%\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","attack_model = RandomForestClassifier(max_depth=7)\n","attack_model.fit(X_train, y_train)\n","\n","attack_predictions = attack_model.predict(X_train)\n","attack_accuracy = accuracy_score(y_train, attack_predictions)\n","print(f'Attack Model Train Accuracy: {attack_accuracy * 100:.2f}%')\n","\n","attack_predictions = attack_model.predict(X_test)\n","attack_accuracy = accuracy_score(y_test, attack_predictions)\n","print(f'Attack Model Test Accuracy: {attack_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T07:09:30.823409Z","iopub.status.busy":"2024-07-02T07:09:30.823037Z","iopub.status.idle":"2024-07-02T07:09:41.830096Z","shell.execute_reply":"2024-07-02T07:09:41.829147Z","shell.execute_reply.started":"2024-07-02T07:09:30.823376Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["privacy regulaztion model\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["privacy regulaztion MIA Predictions: 0.541247\n"]}],"source":["def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            outputs=-np.sort(-outputs , axis=1)\n","            all_outputs.append(torch.Tensor(outputs[:,:4]))\n","            all_labels.append(labels)\n","    return torch.cat(all_outputs), torch.cat(all_labels)\n","\n","def evaluate_mia(model, attack_model, data_loader):\n","    model_outputs, _ = test_model(model, data_loader)\n","    model_features = model_outputs.cpu().view(model_outputs.size(0), -1).numpy()\n","    mia_predictions = attack_model.predict(model_features)\n","    labels = np.zeros(40000)\n","    labels[20000:] = 1\n","    return accuracy_score(mia_predictions, labels)\n","\n","privacy_model_reg = CIFAR10Classifier().to(device)\n","privacy_model_reg.load_state_dict(torch.load('/kaggle/input/my-models/model_privacy_reg.pth'))\n","print('privacy regulaztion model')\n","\n","evaluate_MIA_dataset = torch.utils.data.ConcatDataset([test_testset, Subset(train_dataset,np.arange(len(test_testset)))])\n","evaluate_MIA_loader = DataLoader(evaluate_MIA_dataset, batch_size = 64 , shuffle = False) \n","\n","privacy_reg_model_predictions = evaluate_mia(privacy_model_reg, attack_model, evaluate_MIA_loader)\n","\n","print(f'privacy regulaztion MIA Predictions: {privacy_reg_model_predictions}')"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T07:10:10.996108Z","iopub.status.busy":"2024-07-02T07:10:10.995731Z","iopub.status.idle":"2024-07-02T07:14:51.299917Z","shell.execute_reply":"2024-07-02T07:14:51.297806Z","shell.execute_reply.started":"2024-07-02T07:10:10.996079Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 1.3530470288980503\n","Epoch 10, Loss: 1.1261846908746056\n","Epoch 15, Loss: 0.9864334496446311\n","Epoch 20, Loss: 0.8874182017466512\n","Epoch 25, Loss: 0.7739789630658329\n","Epoch 30, Loss: 0.7326288106152044\n","Epoch 35, Loss: 0.7058166011263387\n","Epoch 40, Loss: 0.6667870883934033\n","Epoch 45, Loss: 0.6137640987531826\n","Epoch 50, Loss: 0.5985221499071335\n","Epoch 55, Loss: 0.5912525315825551\n","Epoch 60, Loss: 0.5663134106241476\n","accuracy :  0.98445\n","accuracy :  0.6117\n","Epoch 5, Loss: 1.3537294508550113\n","Epoch 10, Loss: 1.1419378454311968\n","Epoch 15, Loss: 1.0056898176860505\n","Epoch 20, Loss: 0.8823718763768863\n","Epoch 25, Loss: 0.7938769162653353\n","Epoch 30, Loss: 0.7321221598992332\n","Epoch 35, Loss: 0.6895211843636851\n","Epoch 40, Loss: 0.6678124989945287\n","Epoch 45, Loss: 0.6100456557525232\n","Epoch 50, Loss: 0.5877366406849017\n","Epoch 55, Loss: 0.5791970993192813\n","Epoch 60, Loss: 0.5480531800669223\n","accuracy :  0.9855\n","accuracy :  0.60925\n","Epoch 5, Loss: 1.3362122545607935\n","Epoch 10, Loss: 1.140154929397205\n","Epoch 15, Loss: 1.0006433211195582\n","Epoch 20, Loss: 0.8895957734638129\n","Epoch 25, Loss: 0.7883858831164936\n","Epoch 30, Loss: 0.732116944111955\n","Epoch 35, Loss: 0.692354538665412\n","Epoch 40, Loss: 0.6604314507386936\n","Epoch 45, Loss: 0.6060585937560937\n","Epoch 50, Loss: 0.5877148447135767\n","Epoch 55, Loss: 0.56332455675442\n","Epoch 60, Loss: 0.5532809464504924\n","accuracy :  0.9868\n","accuracy :  0.61\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        scheduler.step()\n","        if epoch % 5 == 4:\n","            print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n","\n","def test_model(model, data_loader):\n","    model.eval()\n","    all_outputs = [[] for i in range(10)]\n","    with torch.no_grad():\n","        correct = 0\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predicted = np.argmax(outputs,axis=1)\n","            correct += np.sum(predicted == labels.cpu().numpy())\n","            all_outputs[labels.item()].append(torch.tensor(outputs))\n","    print('accuracy : ' , correct / len(data_loader.dataset))\n","    return [torch.cat(all_outputs[i]) for i in range(10)]\n","\n","shadow_model_count = 3\n","\n","shadow_train_outputs = []\n","shadow_train_labels = []\n","shadow_test_outputs = []\n","shadow_test_labels = []\n","\n","for _ in range(shadow_model_count):\n","    shadow_train_data, shadow_test_data = train_test_split(train_dataset, test_size=0.5)\n","    shadow_train_loader = DataLoader(shadow_train_data, batch_size=64, shuffle=True)\n","    \n","    model = CIFAR10Classifier().to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=20 , gamma = 0.7)\n","    train_model(model, shadow_train_loader, criterion, optimizer, scheduler , 60)\n","    \n","    shadow_train_loader = DataLoader(shadow_train_data, batch_size=1, shuffle=False)\n","    shadow_test_loader = DataLoader(shadow_test_data, batch_size=1, shuffle=False)\n","    \n","    train_outputs = test_model(model, shadow_train_loader)\n","    shadow_train_outputs.append(train_outputs)\n","\n","    test_outputs = test_model(model, shadow_test_loader)\n","    shadow_test_outputs.append(test_outputs)\n","\n","attack_inputs = []\n","attack_labels = []\n","for i in range(10):\n","    all_train_outputs = torch.cat([shadow_train_outputs[j][i] for j in range(shadow_model_count)])\n","    all_test_outputs = torch.cat([shadow_test_outputs[j][i] for j in range(shadow_model_count)])\n","    attack_inputs.append(torch.cat((all_train_outputs, all_test_outputs)).cpu().numpy())\n","    attack_labels.append(torch.cat((torch.ones(all_train_outputs.shape[0]),torch.zeros(all_test_outputs.shape[0]))).cpu().numpy())"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T07:15:04.321758Z","iopub.status.busy":"2024-07-02T07:15:04.321403Z","iopub.status.idle":"2024-07-02T07:15:22.692422Z","shell.execute_reply":"2024-07-02T07:15:22.691281Z","shell.execute_reply.started":"2024-07-02T07:15:04.321731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Train Accuracy for class 1: 74.91%\n","Attack Model Test Accuracy for class 1: 72.13%\n","Attack Model Train Accuracy for class 2: 70.74%\n","Attack Model Test Accuracy for class 2: 67.43%\n","Attack Model Train Accuracy for class 3: 82.67%\n","Attack Model Test Accuracy for class 3: 80.05%\n","Attack Model Train Accuracy for class 4: 84.60%\n","Attack Model Test Accuracy for class 4: 82.14%\n","Attack Model Train Accuracy for class 5: 82.65%\n","Attack Model Test Accuracy for class 5: 78.80%\n","Attack Model Train Accuracy for class 6: 80.90%\n","Attack Model Test Accuracy for class 6: 77.73%\n","Attack Model Train Accuracy for class 7: 72.65%\n","Attack Model Test Accuracy for class 7: 71.24%\n","Attack Model Train Accuracy for class 8: 73.86%\n","Attack Model Test Accuracy for class 8: 70.83%\n","Attack Model Train Accuracy for class 9: 72.25%\n","Attack Model Test Accuracy for class 9: 67.76%\n","Attack Model Train Accuracy for class 10: 74.44%\n","Attack Model Test Accuracy for class 10: 71.77%\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","attack_models = []\n","for i in range(10):\n","    X_train, X_test, y_train, y_test = train_test_split(attack_inputs[i], attack_labels[i], test_size=0.2)\n","    attack_model = RandomForestClassifier(max_depth=8)\n","    attack_model.fit(X_train, y_train)\n","    attack_models.append(attack_model)\n","    attack_predictions = attack_model.predict(X_train)\n","    attack_accuracy = accuracy_score(y_train, attack_predictions)\n","    print(f'Attack Model Train Accuracy for class {i+1}: {attack_accuracy * 100:.2f}%')\n","    attack_predictions = attack_model.predict(X_test)\n","    attack_accuracy = accuracy_score(y_test, attack_predictions)\n","    print(f'Attack Model Test Accuracy for class {i+1}: {attack_accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T07:22:56.650115Z","iopub.status.busy":"2024-07-02T07:22:56.649599Z","iopub.status.idle":"2024-07-02T07:22:56.676918Z","shell.execute_reply":"2024-07-02T07:22:56.676014Z","shell.execute_reply.started":"2024-07-02T07:22:56.650081Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["baseline model : \n","Baseline Model MIA Predictions: 0.60255\n"]}],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.filterwarnings(\"ignore\", module=\"torch\")\n","\n","def evaluate_mia(model, attack_model, data_loader):\n","    predictions = []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = F.softmax(model(inputs),dim=1).cpu().numpy()\n","            predictions.append(attack_models[labels.item()].predict(outputs)[0])\n","    y_true = np.zeros(40000)\n","    y_true[20000:] = 1\n","    return accuracy_score(y_true, predictions)\n","\n","baseline_model = CIFAR10Classifier().to(device)\n","baseline_model.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","print('baseline model : ')\n","\n","evaluate_MIA_dataset = torch.utils.data.ConcatDataset([test_testset, Subset(train_dataset,np.arange(len(test_testset)))])\n","evaluate_MIA_loader = DataLoader(evaluate_MIA_dataset, batch_size = 1 , shuffle = False) \n","\n","model_predictions = evaluate_mia(baseline_model, attack_model, evaluate_MIA_loader)\n","\n","print(f'Baseline Model MIA Predictions: {model_predictions}')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T05:58:38.507186Z","iopub.status.busy":"2024-07-02T05:58:38.506258Z","iopub.status.idle":"2024-07-02T05:59:41.403270Z","shell.execute_reply":"2024-07-02T05:59:41.402260Z","shell.execute_reply.started":"2024-07-02T05:58:38.507143Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.7124318467140198\n","Epoch 2, Loss: 1.4805127268314362\n","Epoch 3, Loss: 1.3878559970855713\n","Epoch 4, Loss: 1.3129268690109253\n","Train Accuracy: 66.2625%\n","Test Accuracy: 61.44%\n"]}],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T06:07:45.235113Z","iopub.status.busy":"2024-07-02T06:07:45.234416Z","iopub.status.idle":"2024-07-02T06:10:12.657085Z","shell.execute_reply":"2024-07-02T06:10:12.656137Z","shell.execute_reply.started":"2024-07-02T06:07:45.235077Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.6710456382274628\n","Epoch 2, Loss: 1.7742864512443544\n","Epoch 3, Loss: 1.7871973566055297\n","Epoch 4, Loss: 1.7303930786132813\n","Epoch 5, Loss: 1.7140724746704101\n","Train Accuracy: 45.03%\n","Test Accuracy: 44.785%\n","Epoch 6, Loss: 1.5142632371902467\n","Epoch 7, Loss: 1.4439791026115418\n","Epoch 8, Loss: 1.4228863829135894\n","Epoch 9, Loss: 1.4093726710796357\n","Epoch 10, Loss: 1.37401756272316\n","Train Accuracy: 52.535%\n","Test Accuracy: 52.365%\n"]}],"source":["model_privacy = CIFAR10Classifier().to(device)\n","model_privacy.load_state_dict(torch.load('/kaggle/working/model.pth'))\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model_privacy.parameters(), lr=0.001,weight_decay=5e-2)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.5)\n","\n","n_epochs = 10\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model_privacy(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()*inputs.size(0)\n","    epoch_loss=running_loss/len(train_loader.dataset)\n","    scheduler.step()\n","    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n","    if epoch % 5 == 4:\n","        evaluate(model_privacy)\n","torch.save(model_privacy.state_dict(),'model_privacy_reg.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["poisoned_images = []\n","poisoned_labels = []\n","\n","for i in range(len(train_dataset)):\n","    image, label = train_dataset[i]\n","    image_copy = image.clone()\n","    \n","    std_noise = 0.3\n","    noise = np.random.normal(size=(3, 32, 32)) * std_noise\n","    image_copy += noise\n","    poisoned_images.append(image_copy)\n","    poisoned_labels.append(label)\n","    \n","poisoned_images = torch.stack(poisoned_images).to(torch.float32)\n","poisoned_labels = torch.tensor(poisoned_labels)\n","\n","class NoisyDataset(torch.utils.data.Dataset):\n","    def __init__(self, images, labels):\n","        self.images = images\n","        self.labels = labels\n","    \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        return self.images[idx], self.labels[idx]\n","    \n","noisy_dataset = NoisyDataset(poisoned_images, poisoned_labels)\n","noisy_loader = torch.utils.data.DataLoader(noisy_dataset, batch_size = 64, shuffle=True)\n","\n","model_privacy = CIFAR10Classifier().to(device)\n","model_privacy.load_state_dict(torch.load('/kaggle/input/my-models/model_baseline.pth'))\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model_privacy.parameters(), lr=0.0001)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n","\n","n_epochs = 40\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in noisy_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model_privacy(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()*inputs.size(0)\n","    epoch_loss=running_loss/len(noisy_loader.dataset)\n","    scheduler.step(epoch_loss)\n","    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n","    if epoch % 5 == 4:\n","        evaluate(model_privacy)\n","torch.save(model_privacy.state_dict(),'model_privacy_noise.pth')"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T19:09:11.166441Z","iopub.status.busy":"2024-06-25T19:09:11.165575Z","iopub.status.idle":"2024-06-25T19:09:23.368056Z","shell.execute_reply":"2024-06-25T19:09:23.366883Z","shell.execute_reply.started":"2024-06-25T19:09:11.166408Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opacus in /opt/conda/lib/python3.10/site-packages (1.4.1)\n","Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.10/site-packages (from opacus) (1.26.4)\n","Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from opacus) (2.1.2)\n","Requirement already satisfied: scipy>=1.2 in /opt/conda/lib/python3.10/site-packages (from opacus) (1.11.4)\n","Requirement already satisfied: opt-einsum>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from opacus) (3.3.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->opacus) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->opacus) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install opacus"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T19:10:23.118075Z","iopub.status.busy":"2024-06-25T19:10:23.117417Z","iopub.status.idle":"2024-06-25T19:29:36.615527Z","shell.execute_reply":"2024-06-25T19:29:36.614404Z","shell.execute_reply.started":"2024-06-25T19:10:23.118038Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.4772104330033065\n","Epoch 2, Loss: 1.8108938303336501\n","Epoch 3, Loss: 1.8552120708599686\n","Epoch 4, Loss: 1.9156563150823116\n","Train Accuracy: 51.32494079521376%\n","Test Accuracy: 50.635%\n","Epoch 5, Loss: 1.9491818628579378\n","Epoch 6, Loss: 1.9334798613637685\n","Epoch 7, Loss: 1.9002029766261577\n","Epoch 8, Loss: 1.8782178087115289\n","Epoch 9, Loss: 1.8721493428081275\n","Train Accuracy: 53.18032786885246%\n","Test Accuracy: 51.595%\n","Epoch 10, Loss: 1.837915560400486\n","Epoch 11, Loss: 1.8417416595995426\n","Epoch 12, Loss: 1.8423255498617888\n","Epoch 13, Loss: 1.8168749167650937\n","Epoch 14, Loss: 1.8146780160337685\n","Train Accuracy: 53.12209463024851%\n","Test Accuracy: 51.815%\n","Epoch 15, Loss: 1.8465488351583481\n","Epoch 16, Loss: 1.810504553720355\n","Epoch 17, Loss: 1.8204223701387643\n","Epoch 18, Loss: 1.8089205376565456\n","Epoch 19, Loss: 1.822652270373702\n","Train Accuracy: 52.98313714616994%\n","Test Accuracy: 51.825%\n","Epoch 20, Loss: 1.8102970042705535\n","Epoch 21, Loss: 1.8034724285811186\n","Epoch 22, Loss: 1.81887153685987\n","Epoch 23, Loss: 1.8122150491595268\n","Epoch 24, Loss: 1.8176997092753648\n","Train Accuracy: 52.60874984419793%\n","Test Accuracy: 51.875%\n","Epoch 25, Loss: 1.8199076201081277\n","Epoch 26, Loss: 1.8073006374776364\n","Epoch 27, Loss: 1.8245594911545515\n","Epoch 28, Loss: 1.8178741988688707\n","Epoch 29, Loss: 1.826722939386964\n","Train Accuracy: 52.493372680438156%\n","Test Accuracy: 51.865%\n","Epoch 30, Loss: 1.8227768765240908\n","Epoch 31, Loss: 1.8085633683860303\n","Epoch 32, Loss: 1.8077382970958948\n","Epoch 33, Loss: 1.7977260024547577\n","Epoch 34, Loss: 1.807276401001215\n","Train Accuracy: 53.2157779546359%\n","Test Accuracy: 51.865%\n","Epoch 35, Loss: 1.8238940310955047\n","Epoch 36, Loss: 1.8091339246243239\n","Epoch 37, Loss: 1.825648224121332\n","Epoch 38, Loss: 1.8097971544474363\n","Epoch 39, Loss: 1.8093971937716007\n","Train Accuracy: 52.63434404082858%\n","Test Accuracy: 51.87%\n","Epoch 40, Loss: 1.8014531135857106\n","Epoch 41, Loss: 1.8233615351289512\n","Epoch 42, Loss: 1.8242590142905712\n","Epoch 43, Loss: 1.839171054548025\n","Epoch 44, Loss: 1.8416723647475242\n","Train Accuracy: 52.80311929815792%\n","Test Accuracy: 51.87%\n","Epoch 45, Loss: 1.8106554619863628\n","Epoch 46, Loss: 1.8017498110562562\n","Epoch 47, Loss: 1.82379650657475\n","Epoch 48, Loss: 1.800250007107854\n","Epoch 49, Loss: 1.8258903676986695\n","Train Accuracy: 53.033595603846635%\n","Test Accuracy: 51.87%\n","Epoch 50, Loss: 1.8042977185219526\n","Train Accuracy: 52.94878170064644%\n","Test Accuracy: 51.87%\n"]}],"source":["from opacus import PrivacyEngine\n","from opacus.validators import ModuleValidator\n","from torch.optim import lr_scheduler\n","\n","# Define the model, loss function and optimizer\n","model = CIFAR10Classifier().to(device)\n","model.load_state_dict(torch.load('kim_model.pth'))\n","model = ModuleValidator.fix(model)  # Ensure model is compatible with Opacus\n","ModuleValidator.validate(model, strict=False)  # Validate model for DP\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n","\n","# Initialize Privacy Engine\n","privacy_engine = PrivacyEngine()\n","\n","# Attach Privacy Engine\n","model, optimizer, trainloader = privacy_engine.make_private(\n","    module=model,\n","    optimizer=optimizer,\n","    data_loader=trainloader,\n","    noise_multiplier=1.1,\n","    max_grad_norm=1.0,\n",")\n","\n","# Training loop\n","n_epochs = 50\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","    epoch_loss = running_loss / len(trainloader.dataset)\n","    scheduler.step()\n","    if epoch % 5 == 4:\n","        evaluate()\n","    print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n","\n","evaluate()\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T19:30:06.037191Z","iopub.status.busy":"2024-06-25T19:30:06.036530Z","iopub.status.idle":"2024-06-25T19:30:06.047525Z","shell.execute_reply":"2024-06-25T19:30:06.046402Z","shell.execute_reply.started":"2024-06-25T19:30:06.037156Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(),'kim_model_privacy.pth')"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T19:49:00.641024Z","iopub.status.busy":"2024-06-25T19:49:00.640654Z","iopub.status.idle":"2024-06-25T19:54:26.850006Z","shell.execute_reply":"2024-06-25T19:54:26.849013Z","shell.execute_reply.started":"2024-06-25T19:49:00.640997Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Baseline seen predictions: tensor([[-2.4831, -1.2110,  1.7662,  1.3163,  0.6339,  0.7486,  2.0891, -0.2556,\n","         -2.2281, -0.9528],\n","        [ 0.4810, -0.6539,  0.8898, -0.6647,  2.4522, -0.8908, -0.0288, -0.1195,\n","         -0.8072, -1.0280],\n","        [ 0.6598,  2.3060, -1.1582, -0.9929, -0.5425, -1.5622, -0.0236, -1.5446,\n","          0.9652,  1.7259],\n","        [-1.1429, -1.0228,  1.2568, -0.1124,  3.1408, -0.0551,  1.3397,  0.4053,\n","         -2.7172, -1.7134],\n","        [-2.3763, -2.5745,  1.2090,  2.7858,  0.6261,  2.5957,  0.9766,  0.2477,\n","         -2.1618, -1.8062]])\n","Baseline unseen predictions: tensor([[-0.6497, -0.4208,  0.2956,  0.6390,  0.5124,  0.2382, -0.0663, -0.0159,\n","         -0.4348, -0.2425],\n","        [-1.0127, -3.7558,  3.1709,  0.7084,  2.3261,  1.9243, -0.5121,  2.0058,\n","         -3.0185, -2.3087],\n","        [ 1.0644,  1.2032, -1.0783, -1.1890, -1.6148, -1.7972, -1.7374, -1.1041,\n","          2.2862,  3.7336],\n","        [-1.3126, -1.4826,  0.5444,  0.3197,  1.1113,  0.6454,  0.8607,  0.8830,\n","         -1.8343, -0.1489],\n","        [-0.0886,  0.1338,  0.5020,  0.1942,  0.3400, -0.0247,  0.4117,  0.2915,\n","         -1.5809, -0.4613]])\n","Privacy seen predictions: tensor([[-0.0885,  0.0679, -0.0259, -0.1186,  0.0249,  0.0289,  0.0834,  0.0123,\n","          0.0279,  0.0158],\n","        [-0.0891,  0.0663, -0.0137, -0.1263,  0.0314,  0.0318,  0.0680,  0.0178,\n","          0.0107,  0.0099],\n","        [-0.0700,  0.0680, -0.0179, -0.1379,  0.0302,  0.0497,  0.0701,  0.0255,\n","          0.0234,  0.0177],\n","        [-0.0742,  0.0578,  0.0264, -0.1102,  0.0329,  0.0201,  0.0810,  0.0095,\n","          0.0192, -0.0165],\n","        [-0.0731,  0.0452, -0.0261, -0.1144,  0.0231,  0.0200,  0.0821,  0.0131,\n","          0.0151,  0.0059]])\n","Privacy unseen predictions: tensor([[-0.0630,  0.0397, -0.0308, -0.0831,  0.0158,  0.0183,  0.1053,  0.0276,\n","          0.0374,  0.0055],\n","        [-0.0617,  0.0663, -0.0130, -0.1021,  0.0505,  0.0357,  0.0649,  0.0170,\n","          0.0265, -0.0087],\n","        [-0.0952,  0.0648, -0.0366, -0.1183,  0.0535,  0.0323,  0.0855,  0.0401,\n","          0.0582,  0.0089],\n","        [-0.0975,  0.0743, -0.0198, -0.0972,  0.0167,  0.0267,  0.0749,  0.0163,\n","          0.0428,  0.0053],\n","        [-0.0815,  0.0473, -0.0314, -0.1089,  0.0568,  0.0450,  0.0751,  0.0179,\n","          0.0338, -0.0075]])\n","Training baseline attacker...\n","Epoch 1, Loss: 0.5072923460814408\n","Epoch 2, Loss: 0.5025462852558568\n","Epoch 3, Loss: 0.501934667199526\n","Epoch 4, Loss: 0.501230475502905\n","Epoch 5, Loss: 0.5008078886008919\n","Epoch 6, Loss: 0.5008213424903799\n","Epoch 7, Loss: 0.5007483119375036\n","Epoch 8, Loss: 0.5006528765115689\n","Epoch 9, Loss: 0.5006273964030271\n","Epoch 10, Loss: 0.5006697294674695\n","Epoch 11, Loss: 0.5004637019514504\n","Epoch 12, Loss: 0.5002517688945556\n","Epoch 13, Loss: 0.5003091147406621\n","Epoch 14, Loss: 0.5003927209517625\n","Epoch 15, Loss: 0.49992437595865014\n","Epoch 16, Loss: 0.4998144466317928\n","Epoch 17, Loss: 0.4995062332159422\n","Epoch 18, Loss: 0.4990841712676327\n","Epoch 19, Loss: 0.49904720663490465\n","Epoch 20, Loss: 0.4984996299368406\n","Epoch 21, Loss: 0.49829991420186337\n","Epoch 22, Loss: 0.4981479487469466\n","Epoch 23, Loss: 0.49755265841633556\n","Epoch 24, Loss: 0.4974285626899563\n","Epoch 25, Loss: 0.4968287330919249\n","Epoch 26, Loss: 0.4962915822587102\n","Epoch 27, Loss: 0.4961404651784775\n","Epoch 28, Loss: 0.4955725823543961\n","Epoch 29, Loss: 0.4953746364922075\n","Epoch 30, Loss: 0.4946629285278491\n","Epoch 31, Loss: 0.4940804813393247\n","Epoch 32, Loss: 0.4938249052505194\n","Epoch 33, Loss: 0.49316552988779677\n","Epoch 34, Loss: 0.49288472881205786\n","Epoch 35, Loss: 0.49238763251940715\n","Epoch 36, Loss: 0.49181742335998013\n","Epoch 37, Loss: 0.4911724062623386\n","Epoch 38, Loss: 0.49073544909232564\n","Epoch 39, Loss: 0.4901291467368565\n","Epoch 40, Loss: 0.48946775927882274\n","Epoch 41, Loss: 0.4887987805610274\n","Epoch 42, Loss: 0.48795728851646014\n","Epoch 43, Loss: 0.48751219206144625\n","Epoch 44, Loss: 0.48748393770562326\n","Epoch 45, Loss: 0.48747902769197354\n","Epoch 46, Loss: 0.48585986874649634\n","Epoch 47, Loss: 0.4851083641660877\n","Epoch 48, Loss: 0.4842701878157931\n","Epoch 49, Loss: 0.48370122552985806\n","Epoch 50, Loss: 0.48337377777522145\n","Training privacy attacker...\n","Epoch 1, Loss: 0.503198867023792\n","Epoch 2, Loss: 0.5014430127861557\n","Epoch 3, Loss: 0.5013275215794318\n","Epoch 4, Loss: 0.5015633945673303\n","Epoch 5, Loss: 0.5009645806030821\n","Epoch 6, Loss: 0.5011894285240314\n","Epoch 7, Loss: 0.5012356073560428\n","Epoch 8, Loss: 0.5010905956619456\n","Epoch 9, Loss: 0.5009949921760816\n","Epoch 10, Loss: 0.5009530957738177\n","Epoch 11, Loss: 0.5006632663867295\n","Epoch 12, Loss: 0.5007491126818605\n","Epoch 13, Loss: 0.5008188018757642\n","Epoch 14, Loss: 0.5008673318593226\n","Epoch 15, Loss: 0.5007156979671595\n","Epoch 16, Loss: 0.50075887252296\n","Epoch 17, Loss: 0.5004844886804344\n","Epoch 18, Loss: 0.5005916172754131\n","Epoch 19, Loss: 0.5006147840907005\n","Epoch 20, Loss: 0.5006433231893138\n","Epoch 21, Loss: 0.5006497421671928\n","Epoch 22, Loss: 0.5005665752305026\n","Epoch 23, Loss: 0.5005244506011769\n","Epoch 24, Loss: 0.5006927055421733\n","Epoch 25, Loss: 0.5005064916988252\n","Epoch 26, Loss: 0.5005646616087003\n","Epoch 27, Loss: 0.5005909845433171\n","Epoch 28, Loss: 0.5006249975258161\n","Epoch 29, Loss: 0.5004821731086274\n","Epoch 30, Loss: 0.5005557024547554\n","Epoch 31, Loss: 0.500537616601756\n","Epoch 32, Loss: 0.5005210924819732\n","Epoch 33, Loss: 0.5005921493603187\n","Epoch 34, Loss: 0.5006000214445232\n","Epoch 35, Loss: 0.5005487975255084\n","Epoch 36, Loss: 0.5005404558020834\n","Epoch 37, Loss: 0.5005015526074137\n","Epoch 38, Loss: 0.5005864855080786\n","Epoch 39, Loss: 0.5005022079438943\n","Epoch 40, Loss: 0.500674604931018\n","Epoch 41, Loss: 0.500421944254877\n","Epoch 42, Loss: 0.5005118808663204\n","Epoch 43, Loss: 0.5005474509677289\n","Epoch 44, Loss: 0.5005235033811702\n","Epoch 45, Loss: 0.5005336193247476\n","Epoch 46, Loss: 0.5004755756123586\n","Epoch 47, Loss: 0.5005720764188833\n","Epoch 48, Loss: 0.5005341158871154\n","Epoch 49, Loss: 0.5003698671847983\n","Epoch 50, Loss: 0.500608989245527\n","Baseline Attacker Accuracy: 80.66%\n","Privacy Attacker Accuracy: 80.00%\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset, random_split\n","from torchvision import datasets, transforms\n","import numpy as np\n","from opacus import PrivacyEngine\n","from opacus.validators import ModuleValidator\n","from torch.optim import lr_scheduler\n","import torch.optim as optim\n","from copy import deepcopy\n","\n","# Prepare the data\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Split training data into seen (80%) and unseen (20%)\n","num_train = len(trainset)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(0.8 * num_train))\n","seen_indices, unseen_indices = indices[:split], indices[split:]\n","\n","seen_set = Subset(trainset, seen_indices)\n","unseen_set = Subset(trainset, unseen_indices)\n","\n","# Loaders\n","seen_loader = DataLoader(seen_set, batch_size=32, shuffle=True)\n","unseen_loader = DataLoader(unseen_set, batch_size=32, shuffle=True)\n","test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n","\n","# Define the model, loss function and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load and evaluate the baseline model\n","baseline_model = CIFAR10Classifier().to(device)\n","baseline_model.load_state_dict(torch.load('/kaggle/working/kim_model.pth'))\n","baseline_model.eval()\n","\n","# Load and evaluate the privacy-enhanced model\n","privacy_model = CIFAR10Classifier().to(device)\n","privacy_model.load_state_dict(torch.load('/kaggle/working/kim_model_privacy.pth', map_location=device), strict=False)\n","privacy_model.eval()\n","\n","# Helper function to get model predictions\n","def get_predictions(model, loader):\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for inputs, _ in loader:\n","            inputs = inputs.to(device)\n","            outputs = model(inputs)\n","            predictions.append(outputs.detach().cpu())\n","    return torch.cat(predictions)\n","\n","# Get predictions from both models on seen and unseen data\n","baseline_seen_preds = get_predictions(baseline_model, seen_loader)\n","baseline_unseen_preds = get_predictions(baseline_model, unseen_loader)\n","\n","privacy_seen_preds = get_predictions(privacy_model, seen_loader)\n","privacy_unseen_preds = get_predictions(privacy_model, unseen_loader)\n","\n","\n","# Prepare the attacker data\n","def prepare_attacker_data(seen_preds, unseen_preds):\n","    seen_labels = torch.ones(seen_preds.size(0))\n","    unseen_labels = torch.zeros(unseen_preds.size(0))\n","    return torch.cat([seen_preds, unseen_preds]), torch.cat([seen_labels, unseen_labels])\n","\n","baseline_attacker_data, baseline_attacker_labels = prepare_attacker_data(baseline_seen_preds, baseline_unseen_preds)\n","privacy_attacker_data, privacy_attacker_labels = prepare_attacker_data(privacy_seen_preds, privacy_unseen_preds)\n","\n","# Define a more complex Attacker model\n","class ComplexAttackerModel(nn.Module):\n","    def __init__(self, input_size):\n","        super(ComplexAttackerModel, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = torch.sigmoid(self.fc4(x))\n","        return x\n","\n","# Train the attacker model\n","def train_attacker(attacker_model, data, labels):\n","    attacker_model = attacker_model.to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(attacker_model.parameters(), lr=0.001)\n","\n","    dataset = torch.utils.data.TensorDataset(data, labels)\n","    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","    attacker_model.train()\n","    for epoch in range(50):  # Train for 50 epochs\n","        running_loss = 0.0\n","        for inputs, targets in loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = attacker_model(inputs)\n","            loss = criterion(outputs.squeeze(), targets)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(loader)}\")\n","\n","    return attacker_model\n","\n","# Instantiate and train attacker models\n","input_size = baseline_attacker_data.size(1)\n","baseline_attacker = ComplexAttackerModel(input_size)\n","privacy_attacker = ComplexAttackerModel(input_size)\n","\n","print(\"Training baseline attacker...\")\n","baseline_attacker = train_attacker(baseline_attacker, baseline_attacker_data, baseline_attacker_labels)\n","\n","print(\"Training privacy attacker...\")\n","privacy_attacker = train_attacker(privacy_attacker, privacy_attacker_data, privacy_attacker_labels)\n","\n","# Evaluate the attacker models\n","def evaluate_attacker(attacker_model, data, labels):\n","    attacker_model.eval()\n","    with torch.no_grad():\n","        outputs = attacker_model(data.to(device)).squeeze()\n","        preds = (outputs > 0.5).float().cpu()\n","        accuracy = (preds == labels).float().mean().item()\n","    return accuracy\n","\n","baseline_attacker_accuracy = evaluate_attacker(baseline_attacker, baseline_attacker_data, baseline_attacker_labels)\n","privacy_attacker_accuracy = evaluate_attacker(privacy_attacker, privacy_attacker_data, privacy_attacker_labels)\n","\n","print(f\"Baseline Attacker Accuracy: {baseline_attacker_accuracy * 100:.2f}%\")\n","print(f\"Privacy Attacker Accuracy: {privacy_attacker_accuracy * 100:.2f}%\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5297775,"sourceId":8808357,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
